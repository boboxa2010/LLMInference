{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/boboxa/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mivanboboshko888\u001b[0m (\u001b[33mivanboboshko888-hse-university4375\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/boboxa/llm_inference/kv-cache/wandb/run-20250429_084311-c0hp9dzx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark/runs/c0hp9dzx' target=\"_blank\">transformer-kv-cache-comparison</a></strong> to <a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark' target=\"_blank\">https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark/runs/c0hp9dzx' target=\"_blank\">https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark/runs/c0hp9dzx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:00<00:00, 15.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:13<00:00, 18.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch size: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:40<00:00, 25.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch size: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:25<00:00, 36.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▄▄▄▄▄▄▄█████████▁▁▂█</td></tr><tr><td>input_length</td><td>▁▁▂▂▄██▁▁▂▂▂▂▂▄████▁▁▂▂▂▄██▁▁▁▂▄▄▄▄██▁▂▄</td></tr><tr><td>latency/with_kv_cache</td><td>▄▁▁▁▁▁▁▁▁█▇▄▁▁█▁▁▁▁▁▁▁▁▁▁█▂▇▂▆▂▇▂▁▁▁▂▂▂▂</td></tr><tr><td>latency/without_kv_cache</td><td>▁▁▁▁▁▂▂▁▂▁▁▂▁▂▁▃▃▃▂▂▃▂▃▃▃▄▅▄▃▃▃▄▄▄▄▅▆▅▅█</td></tr><tr><td>memory/with_kv_cache</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▁▁▁▁▂▂▂▂▃▄▂▂▂▃▃▄▄██</td></tr><tr><td>memory/without_kv_cache</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▂▂▂▂▃▃▃▃▄▄▃▄▄▄▆▆████</td></tr><tr><td>memory_increase_ratio</td><td>█▇██▆▅▇▇▄▄▅▇▁▁▃▇</td></tr><tr><td>repeat</td><td>█▁▃▆▅▅█▁█▅▁▆▃▆▃▅▅█▆▁▁▅▅▁▆█▁▅▆▅▆██▅█▁▅▆▁█</td></tr><tr><td>speedup_ratio</td><td>▁▁▁▁▁▁▂▃▁▂▂▄▂▄▄█</td></tr><tr><td>throughput/with_kv_cache</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▂▃▃▂▃▃▃▃▃▃▄▄▃▄▄▄▃██████████</td></tr><tr><td>throughput/without_kv_cache</td><td>▂▁▂▂▂▂▁▂▂▂▁▂▃▆▃▅▂▂▂▂▂▂▅█▅▄▄▄▄▆▃▃▃▂▇▅▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_size</td><td>16</td></tr><tr><td>input_length</td><td>200</td></tr><tr><td>latency/with_kv_cache</td><td>0.79712</td></tr><tr><td>latency/without_kv_cache</td><td>8.51889</td></tr><tr><td>memory/with_kv_cache</td><td>1.30931</td></tr><tr><td>memory/without_kv_cache</td><td>1.39153</td></tr><tr><td>memory_increase_ratio</td><td>0.94091</td></tr><tr><td>repeat</td><td>4</td></tr><tr><td>speedup_ratio</td><td>10.59228</td></tr><tr><td>throughput/with_kv_cache</td><td>2007.22518</td></tr><tr><td>throughput/without_kv_cache</td><td>187.81782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transformer-kv-cache-comparison</strong> at: <a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark/runs/c0hp9dzx' target=\"_blank\">https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark/runs/c0hp9dzx</a><br> View project at: <a href='https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark' target=\"_blank\">https://wandb.ai/ivanboboshko888-hse-university4375/kv-cache-benchmark</a><br>Synced 5 W&B file(s), 6 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250429_084311-c0hp9dzx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "wandb.login(key='TOKEN')\n",
    "\n",
    "wandb.init(\n",
    "    project=\"kv-cache-benchmark\",\n",
    "    name=\"transformer-kv-cache-comparison\",\n",
    "    config={\n",
    "        \"model_name\": \"facebook/opt-125m\",\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"input_lengths\": [10, 50, 100, 200],\n",
    "        \"output_tokens\": 100,\n",
    "        \"batch_sizes\": [1, 4, 8, 16],\n",
    "        \"n_repeats\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.model_name).to(config.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def run_inference(prompt, output_length, use_kv_cache=True, batch_size=1):\n",
    "    inputs = tokenizer(prompt, padding=True, return_tensors=\"pt\").to(config.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=output_length,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache=use_kv_cache\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    latency = end_time - start_time\n",
    "    throughput = (output_length * batch_size) / latency\n",
    "    \n",
    "    if config.device == \"cuda\":\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    else:\n",
    "        memory_usage = 0\n",
    "    \n",
    "    return latency, throughput, memory_usage\n",
    "\n",
    "columns = [\"batch_size\", \"input_length\", \"kv_cache\", \"latency\", \"throughput\", \"memory_usage\", \"repeat\"]\n",
    "table = wandb.Table(columns=columns)\n",
    "\n",
    "for batch_size in config.batch_sizes:\n",
    "    print(f\"Testing batch size: {batch_size}\")\n",
    "    \n",
    "    for input_length in tqdm(config.input_lengths):\n",
    "        prompts = [\n",
    "            \" \".join([\"test\"] * input_length) for _ in range(batch_size)\n",
    "        ]\n",
    "        \n",
    "        for repeat in range(config.n_repeats):\n",
    "            if config.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            latency, throughput, memory = run_inference(\n",
    "                prompts, config.output_tokens, use_kv_cache=True, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            table.add_data(batch_size, input_length, \"with_kv_cache\", \n",
    "                          latency, throughput, memory, repeat)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"latency/with_kv_cache\": latency,\n",
    "                \"throughput/with_kv_cache\": throughput,\n",
    "                \"memory/with_kv_cache\": memory,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"input_length\": input_length,\n",
    "                \"repeat\": repeat\n",
    "            })\n",
    "        \n",
    "        for repeat in range(config.n_repeats):\n",
    "            if config.device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                \n",
    "            latency, throughput, memory = run_inference(\n",
    "                prompts, config.output_tokens, use_kv_cache=False, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            table.add_data(batch_size, input_length, \"without_kv_cache\", \n",
    "                          latency, throughput, memory, repeat)\n",
    "            \n",
    "            wandb.log({\n",
    "                \"latency/without_kv_cache\": latency,\n",
    "                \"throughput/without_kv_cache\": throughput,\n",
    "                \"memory/without_kv_cache\": memory,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"input_length\": input_length,\n",
    "                \"repeat\": repeat\n",
    "            })\n",
    "\n",
    "wandb.log({\"results_table\": table})\n",
    "\n",
    "for batch_size in config.batch_sizes:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    batch_data = [row for row in table.data if row[0] == batch_size]\n",
    "\n",
    "    data_grouped = {}\n",
    "    for row in batch_data:\n",
    "        key = (row[1], row[2])\n",
    "        if key not in data_grouped:\n",
    "            data_grouped[key] = []\n",
    "        data_grouped[key].append((row[3], row[4], row[5]))\n",
    "    \n",
    "    avg_data = {}\n",
    "    for key, values in data_grouped.items():\n",
    "        latencies, throughputs, memories = zip(*values)\n",
    "        avg_data[key] = (np.mean(latencies), np.mean(throughputs), np.mean(memories))\n",
    "    \n",
    "    input_lengths = sorted(set(key[0] for key in avg_data.keys()))\n",
    "    with_kv_cache_latency = [avg_data.get((il, \"with_kv_cache\"), (0, 0, 0))[0] for il in input_lengths]\n",
    "    without_kv_cache_latency = [avg_data.get((il, \"without_kv_cache\"), (0, 0, 0))[0] for il in input_lengths]\n",
    "    \n",
    "    with_kv_cache_throughput = [avg_data.get((il, \"with_kv_cache\"), (0, 0, 0))[1] for il in input_lengths]\n",
    "    without_kv_cache_throughput = [avg_data.get((il, \"without_kv_cache\"), (0, 0, 0))[1] for il in input_lengths]\n",
    "    \n",
    "    with_kv_cache_memory = [avg_data.get((il, \"with_kv_cache\"), (0, 0, 0))[2] for il in input_lengths]\n",
    "    without_kv_cache_memory = [avg_data.get((il, \"without_kv_cache\"), (0, 0, 0))[2] for il in input_lengths]\n",
    "    \n",
    "    ax1.plot(input_lengths, with_kv_cache_latency, 'o-', label='With KV Cache')\n",
    "    ax1.plot(input_lengths, without_kv_cache_latency, 'o-', label='Without KV Cache')\n",
    "    ax1.set_title(f'Latency (Batch Size: {batch_size})')\n",
    "    ax1.set_xlabel('Input Length (tokens)')\n",
    "    ax1.set_ylabel('Latency (seconds)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(input_lengths, with_kv_cache_throughput, 'o-', label='With KV Cache')\n",
    "    ax2.plot(input_lengths, without_kv_cache_throughput, 'o-', label='Without KV Cache')\n",
    "    ax2.set_title(f'Throughput (Batch Size: {batch_size})')\n",
    "    ax2.set_xlabel('Input Length (tokens)')\n",
    "    ax2.set_ylabel('Throughput (tokens/second)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    ax3.plot(input_lengths, with_kv_cache_memory, 'o-', label='With KV Cache')\n",
    "    ax3.plot(input_lengths, without_kv_cache_memory, 'o-', label='Without KV Cache')\n",
    "    ax3.set_title(f'Memory Usage (Batch Size: {batch_size})')\n",
    "    ax3.set_xlabel('Input Length (tokens)')\n",
    "    ax3.set_ylabel('Memory Usage (GB)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({f\"comparison_batch_{batch_size}\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "speedup_data = []\n",
    "memory_increase_data = []\n",
    "\n",
    "for batch_size in config.batch_sizes:\n",
    "    for input_length in config.input_lengths:\n",
    "\n",
    "        with_kv = [(row[3], row[5]) for row in table.data \n",
    "                  if row[0] == batch_size and row[1] == input_length and row[2] == \"with_kv_cache\"]\n",
    "        without_kv = [(row[3], row[5]) for row in table.data \n",
    "                     if row[0] == batch_size and row[1] == input_length and row[2] == \"without_kv_cache\"]\n",
    "        \n",
    "        if with_kv and without_kv:\n",
    "            avg_with_latency = np.mean([x[0] for x in with_kv])\n",
    "            avg_without_latency = np.mean([x[0] for x in without_kv])\n",
    "            avg_with_memory = np.mean([x[1] for x in with_kv])\n",
    "            avg_without_memory = np.mean([x[1] for x in without_kv])\n",
    "            \n",
    "            speedup = avg_without_latency / avg_with_latency if avg_with_latency > 0 else 0\n",
    "            memory_increase = avg_with_memory / avg_without_memory if avg_without_memory > 0 else 0\n",
    "            \n",
    "            speedup_data.append({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"input_length\": input_length,\n",
    "                \"speedup_ratio\": speedup\n",
    "            })\n",
    "            \n",
    "            memory_increase_data.append({\n",
    "                \"batch_size\": batch_size,\n",
    "                \"input_length\": input_length,\n",
    "                \"memory_increase_ratio\": memory_increase\n",
    "            })\n",
    "            \n",
    "            wandb.log({\n",
    "                \"speedup_ratio\": speedup,\n",
    "                \"memory_increase_ratio\": memory_increase,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"input_length\": input_length\n",
    "            })\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for batch_size in config.batch_sizes:\n",
    "    batch_speedup = [(item[\"input_length\"], item[\"speedup_ratio\"]) \n",
    "                     for item in speedup_data if item[\"batch_size\"] == batch_size]\n",
    "    batch_memory = [(item[\"input_length\"], item[\"memory_increase_ratio\"]) \n",
    "                    for item in memory_increase_data if item[\"batch_size\"] == batch_size]\n",
    "    \n",
    "\n",
    "    batch_speedup.sort(key=lambda x: x[0])\n",
    "    batch_memory.sort(key=lambda x: x[0])\n",
    "    \n",
    "\n",
    "    if batch_speedup:\n",
    "        x, y = zip(*batch_speedup)\n",
    "        ax1.plot(x, y, 'o-', label=f'Batch {batch_size}')\n",
    "    \n",
    "    if batch_memory:\n",
    "        x, y = zip(*batch_memory)\n",
    "        ax2.plot(x, y, 'o-', label=f'Batch {batch_size}')\n",
    "\n",
    "ax1.set_title('Speedup Ratio (Without KV / With KV)')\n",
    "ax1.set_xlabel('Input Length (tokens)')\n",
    "ax1.set_ylabel('Speedup Ratio')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_title('Memory Usage Increase (With KV / Without KV)')\n",
    "ax2.set_xlabel('Input Length (tokens)')\n",
    "ax2.set_ylabel('Memory Increase Ratio')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "wandb.log({\"summary_ratios\": wandb.Image(fig)})\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43776826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca120f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='TOKEN')\n",
    "\n",
    "wandb.init(\n",
    "    project=\"kv-cache-benchmark\",\n",
    "    name=\"transformer-kv-cache-comparison\",\n",
    "    config={\n",
    "        \"model_name\": \"facebook/opt-125m\",\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"input_lengths\": [10, 50, 100, 200],\n",
    "        \"output_tokens\": 100,\n",
    "        \"batch_sizes\": [1, 4, 8, 16],\n",
    "        \"n_repeats\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d22619",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(config.model_name).to(config.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
